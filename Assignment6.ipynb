{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4WeHxhHCOqyP0oZJ+1PrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrishikreddy/nlp/blob/main/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qbVlvqccVv6L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (a) Preprocessing of the Data\n",
        "# Load data from keras.datasets\n",
        "max_words = 10000  # Limit to 10,000 most frequent words\n",
        "max_len = 200  # Maximum length of each sequence\n",
        "\n",
        "# Load IMDB dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "-Vbn4vtyWFkM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (b) Divide data into training and testing data set\n",
        "# Here, we are using the pre-split data from IMDB\n",
        "# To further split training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# (c) Build the Gated Recurrent Units (GRU) Model\n",
        "embedding_dim = 128  # Dimension of embedding layer\n",
        "gru_units = 64  # Number of GRU units\n"
      ],
      "metadata": {
        "id": "yny7qdk1XVXW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(GRU(units=gru_units, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification (positive/negative sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsaSwpNRXZ3B",
        "outputId": "7144114b-d736-4c54-fd0b-3b0d16d24d62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# (d) Training the GRU Model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOSryG21XdDc",
        "outputId": "96da6b9b-0f2b-40bf-d9d0-0cef5015b1b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 330ms/step - accuracy: 0.6902 - loss: 0.5427 - val_accuracy: 0.8442 - val_loss: 0.3609\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 331ms/step - accuracy: 0.9063 - loss: 0.2362 - val_accuracy: 0.8782 - val_loss: 0.3029\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 316ms/step - accuracy: 0.9431 - loss: 0.1606 - val_accuracy: 0.8748 - val_loss: 0.3203\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 324ms/step - accuracy: 0.9620 - loss: 0.1103 - val_accuracy: 0.8740 - val_loss: 0.3717\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 319ms/step - accuracy: 0.9758 - loss: 0.0760 - val_accuracy: 0.8690 - val_loss: 0.4426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (e) Text Generation Using the Trained Model\n",
        "# For text generation, you need a trained model that can generate sequences.\n",
        "# Here, we'll implement a simple method for text generation based on the GRU model:\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_len):\n",
        "    for _ in range(50):  # Generate 50 words\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == np.argmax(predicted):\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Example: Generate text (Note: Requires a text-based model and tokenizer)\n",
        "# tokenizer = Tokenizer(num_words=max_words)\n",
        "# tokenizer.fit_on_texts(your_text_data)\n",
        "# print(generate_text(model, tokenizer, seed_text=\"This movie\", max_sequence_len=max_len))\n",
        "\n",
        "# (f) Evaluate Model’s accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odco0PCLZX3d",
        "outputId": "11679829-c3cf-4164-ca30-2d76bf9c1ded"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.8593 - loss: 0.4825\n",
            "Test Accuracy: 85.92%\n"
          ]
        }
      ]
    }
  ]
}